{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Library Loaded\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from dateutil import parser\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix,classification_report,accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import pickle\n",
    "from lightgbm import LGBMClassifier\n",
    "print('Library Loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import requests\n",
    "def iterable_to_stream(iterable, buffer_size=io.DEFAULT_BUFFER_SIZE):\n",
    "    class IterStream(io.RawIOBase):\n",
    "        def __init__(self):\n",
    "            self.leftover = None\n",
    "        def readable(self):\n",
    "            return True\n",
    "        def readinto(self, b):\n",
    "            try:\n",
    "                l = len(b)\n",
    "                chunk = self.leftover or next(iterable)\n",
    "                output, self.leftover = chunk[:l], chunk[l:] # chunk, up to limit\n",
    "                b[:len(output)] = output \n",
    "                return len(output)\n",
    "            except StopIteration:\n",
    "                return 0\n",
    "    return io.BufferedReader(IterStream(), buffer_size=buffer_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/cosmicudemy/ML_Casestudies/master/diabetes/diabetes.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(url, stream=True)\n",
    "next(response.iter_content())\n",
    "df = pd.read_csv(iterable_to_stream(response.iter_content()), sep=',')\n",
    "df.rename(columns={\"regnancies\":\"Pregnancies\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 768 entries, 0 to 767\n",
      "Data columns (total 9 columns):\n",
      " #   Column                    Non-Null Count  Dtype  \n",
      "---  ------                    --------------  -----  \n",
      " 0   Pregnancies               768 non-null    int64  \n",
      " 1   Glucose                   768 non-null    int64  \n",
      " 2   BloodPressure             768 non-null    int64  \n",
      " 3   SkinThickness             768 non-null    int64  \n",
      " 4   Insulin                   768 non-null    int64  \n",
      " 5   BMI                       768 non-null    float64\n",
      " 6   DiabetesPedigreeFunction  768 non-null    float64\n",
      " 7   Age                       768 non-null    int64  \n",
      " 8   Outcome                   768 non-null    int64  \n",
      "dtypes: float64(2), int64(7)\n",
      "memory usage: 54.1 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for sagemaker use: move outcome column to position 0\n",
    "def move_column_inplace(df, col, pos):\n",
    "    col = df.pop(col)\n",
    "    df.insert(pos, col.name, col)\n",
    "    return df\n",
    "\n",
    "df2 = move_column_inplace(df, \"Outcome\", 0)\n",
    "df2.to_csv(\"./diabetes.csv\", index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RangeIndex(start=0, stop=768, step=1)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set : 614\n",
      "Test Set : 154\n",
      "Training labels : 614\n",
      "Test Labels : 154\n"
     ]
    }
   ],
   "source": [
    "X = df.drop('Outcome',axis=1) # predictor feature coloumns\n",
    "y = df.Outcome\n",
    "\n",
    "X_train , X_test , y_train , y_test = train_test_split(X, y, test_size = 0.20, random_state = 11)\n",
    "\n",
    "print('Training Set :',len(X_train))\n",
    "print('Test Set :',len(X_test))\n",
    "print('Training labels :',len(y_train))\n",
    "print('Test Labels :',len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FitModel(X_train,y_train,X_test,y_test,algo_name,algorithm,gridSearchParams,cv):\n",
    "    np.random.seed(10)\n",
    "   \n",
    "#     x_train, x_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n",
    "\n",
    "#     train = pd.concat([y_train, x_train], axis=1)\n",
    "#     train.to_csv(\"./train.csv\", index=False, header=False)\n",
    "#     y_train.to_csv(\"./Y-train.csv\", index=False, header=False)\n",
    "\n",
    "#     test = pd.concat([y_test, x_test], axis=1)\n",
    "#     test.to_csv(\"./test.csv\", index=False, header=False)\n",
    "#     y_test.to_csv(\"./Y-test.csv\", index=False, header=False)\n",
    "\n",
    "    \n",
    "    grid = GridSearchCV(\n",
    "        estimator=algorithm,\n",
    "        param_grid=gridSearchParams,\n",
    "        cv=cv, scoring='accuracy', verbose=1, n_jobs=-1)\n",
    "    \n",
    "    \n",
    "    grid_result = grid.fit(X_train, y_train)\n",
    "    best_params = grid_result.best_params_\n",
    "    pred = grid_result.predict(X_test)\n",
    "    cm = confusion_matrix(y_test, pred)\n",
    "   # metrics =grid_result.gr\n",
    "    print(pred)\n",
    "    pickle.dump(grid_result,open(algo_name+'.pkl','wb'))\n",
    "   \n",
    "    print('Best Params :',best_params)\n",
    "    print('Classification Report :',classification_report(y_test,pred))\n",
    "    print('Accuracy Score : ' + str(accuracy_score(y_test,pred)))\n",
    "    print('Confusion Matrix : \\n', cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SMOTE works by resampling x and y with replacement\n",
    "from imblearn.over_sampling import SMOTE\n",
    "sm = SMOTE(random_state=867)\n",
    "X_res_OS, Y_res_OS = sm.fit_resample(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set : 800\n",
      "Test Set : 200\n",
      "Training labels : 800\n",
      "Test Labels : 200\n"
     ]
    }
   ],
   "source": [
    "X_train , X_test , y_train , y_test = train_test_split(X_res_OS, Y_res_OS, test_size = 0.20, random_state = 11)\n",
    "\n",
    "print('Training Set :',len(X_train))\n",
    "print('Test Set :',len(X_test))\n",
    "print('Training labels :',len(y_train))\n",
    "print('Test Labels :',len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "fill = SimpleImputer(missing_values = 0, strategy=\"mean\")\n",
    "\n",
    "X_train = fill.fit_transform(X_train)\n",
    "X_test = fill.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logit regression after oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logit\n",
    "penalty = [\"l1\", \"l2\"]\n",
    "C = np.logspace(0, 4, 10)\n",
    "hyperparameters = dict(C=C, penalty=penalty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "[1 0 0 0 1 1 0 0 1 0 1 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0 1 1 0 1 0 1\n",
      " 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 1 1 1 0 0 0 0 0 1 1 0 1 1\n",
      " 0 1 1 1 1 1 1 1 0 1 1 1 0 0 0 0 0 1 1 0 1 0 0 1 1 0 0 0 0 1 0 1 1 0 1 1 1\n",
      " 0 0 0 0 1 1 0 0 1 1 1 1 1 1 0 0 1 1 1 0 0 0 0 0 1 1 0 0 0 1 1 1 0 0 1 0 0\n",
      " 0 0 1 0 1 0 1 1 1 1 1 0 0 0 1 1 0 0 1 1 0 1 0 0 1 1 1 1 0 1 1 1 1 1 0 0 1\n",
      " 0 0 1 0 1 1 0 1 1 0 1 0 1 1 0]\n",
      "Best Params : {'C': 10000.0, 'penalty': 'l2'}\n",
      "Classification Report :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.72      0.73       109\n",
      "           1       0.68      0.69      0.68        91\n",
      "\n",
      "    accuracy                           0.71       200\n",
      "   macro avg       0.71      0.71      0.71       200\n",
      "weighted avg       0.71      0.71      0.71       200\n",
      "\n",
      "Accuracy Score : 0.71\n",
      "Confusion Matrix : \n",
      " [[79 30]\n",
      " [28 63]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/christophermartin/DocumentsNoCloud/repos/cmcode/deployml/aws_deploy_diabetes/venvdiab/lib/python3.8/site-packages/sklearn/model_selection/_search.py:918: UserWarning: One or more of the test scores are non-finite: [    nan 0.72875     nan 0.73        nan 0.73375     nan 0.72625     nan\n",
      " 0.73125     nan 0.725       nan 0.72875     nan 0.73125     nan 0.73125\n",
      "     nan 0.73375]\n",
      "  warnings.warn(\n",
      "/Users/christophermartin/DocumentsNoCloud/repos/cmcode/deployml/aws_deploy_diabetes/venvdiab/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "FitModel(X_train,y_train,X_test,y_test,'LR_OS',LogisticRegression(),hyperparameters,cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logit\n",
    "penalty = [\"l1\", \"l2\"]\n",
    "C = np.logspace(0, 4, 10)\n",
    "hyperparameters = dict(C=C, penalty=penalty)\n",
    "FitModel(X_train,y_train,X_test,y_test,'LogisticRegression',LogisticRegression(),hyperparameters,cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/christophermartin/DocumentsNoCloud/repos/cmcode/deployml/aws_deploy_diabetes/venvdiab/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:34:40] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0 0 0 0 1 1 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 0 1 0 0 0 1 0 1\n",
      " 0 1 0 0 0 1 1 1 0 1 0 1 0 0 0 1 1 1 1 1 1 1 0 0 1 0 1 0 0 1 0 1 0 1 0 0 1\n",
      " 0 1 1 1 0 0 1 1 0 1 1 1 0 0 0 0 1 1 0 0 1 0 0 1 1 0 0 0 0 1 0 1 1 0 1 1 0\n",
      " 0 0 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 0 0 0 1 0 1 1 1 0 0 1 1 0 0 1 1 0 0\n",
      " 0 0 1 0 1 0 1 1 1 1 1 0 0 0 1 0 0 0 1 0 0 1 0 0 1 1 1 1 0 0 0 1 1 1 0 0 1\n",
      " 0 1 0 0 1 1 0 1 1 0 0 0 1 0 0]\n",
      "Best Params : {'learning_rate': 0.05, 'max_depth': 3, 'n_estimators': 1200}\n",
      "Classification Report :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.79      0.80       109\n",
      "           1       0.75      0.77      0.76        91\n",
      "\n",
      "    accuracy                           0.78       200\n",
      "   macro avg       0.78      0.78      0.78       200\n",
      "weighted avg       0.78      0.78      0.78       200\n",
      "\n",
      "Accuracy Score : 0.78\n",
      "Confusion Matrix : \n",
      " [[86 23]\n",
      " [21 70]]\n"
     ]
    }
   ],
   "source": [
    "# To fit XGboost: grid search over this dict of param values.\n",
    "param = {\n",
    "    \"n_estimators\": [100, 600, 1200],  # why should this cross 2 orders of magnitude?\n",
    "    \"max_depth\": [2, 3, 4, 5],  # depth of sequential classifiers\n",
    "    \"learning_rate\": np.arange(0.01, 0.1, 0.01).tolist(),  # controls the greediness\n",
    "}\n",
    "FitModel(X_train,y_train,X_test,y_test,'XGBoost_OS',XGBClassifier(),param,cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# To fit random forest:\n",
    "param = {\n",
    "    \"n_estimators\": [100, 500, 1000, 1500, 2000]\n",
    "}\n",
    "FitModel(X_train,y_train,X_test,y_test,'RandomForest',RandomForestClassifier(),param,cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvdiab",
   "language": "python",
   "name": "venvdiab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
